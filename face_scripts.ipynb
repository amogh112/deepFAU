{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and setting folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, glob, argparse, math, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bcolz import carray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_DISFA_data = \"/media/amogh/Stuff/CMU/datasets/DISFA_data/\"\n",
    "folder_DISFA_FAU = \"/media/amogh/Stuff/CMU/datasets/DISFA_data/ActionUnit_Labels/\"\n",
    "folder_DISFA_FAU_summary = \"DISFA_FAUs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a dictionary with positives and negatives for each subject and frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function gives a dictionary in which all positives and negatives are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary in the form: {'SN001':{'positives': [1,2,3],'negatives':[4,5,6,7] }}\n",
    "# ie corresponding to each subject a dictionary which contains list frame nos which are positives and \n",
    "def getDISFAFramesDictionary(folder_DISFA_FAU_summary, fau_no, fau_thresh):\n",
    "    df_fau = pd.read_csv(folder_DISFA_FAU_summary + \"{}/\".format(fau_thresh) + \"FAU{}.csv\".format(fau_no))\n",
    "    df_positives = df_fau.filter(regex=\"^((?!neg).)*$\",axis=1)\n",
    "    df_negatives = df_fau.filter(like=\"neg\",axis=1) \n",
    "    list_subjects = df_positives.columns.values\n",
    "    fau_dict = {}\n",
    "    for subj in list_subjects:\n",
    "        fau_dict[subj] = {'positives':[], 'negatives':[]}\n",
    "        fau_dict[subj]['positives'] = [f for f in df_positives[subj].values if not math.isnan(f)]\n",
    "        fau_dict[subj]['negatives'] = [f for f in df_negatives[\"{}_neg\".format(subj)].values if not math.isnan(f)]\n",
    "    return fau_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To have number of positives and negatives equal in number, let's have a dictionary in which the positives and the negatives corresponding to each category are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equaliseDictionary(fau_dict):\n",
    "    for subj in fau_dict.keys():\n",
    "        number_positives = len(fau_dict[subj]['positives'])\n",
    "        fau_dict[subj]['negatives'] = random.sample(fau_dict[subj]['negatives'], number_positives)\n",
    "    return fau_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test and train folds of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary with keys as fold_0,fold_1,...,test\n",
    "# make sure number of folds exactly divide the train subjects\n",
    "def getTrainTestFolds (fau_dict, no_folds, no_test_subjects):\n",
    "    list_subjects = fau_dict.keys()\n",
    "    no_train_subjects = len(list_subjects) - no_test_subjects\n",
    "    random.shuffle(list_subjects)\n",
    "    test_subjects = list_subjects[-no_test_subjects:]\n",
    "    train_subjects = list_subjects[:-no_test_subjects]\n",
    "    dict_folds = {'test':{}}\n",
    "    # putting train and test subjects in new dictionary\n",
    "    for subj in test_subjects:\n",
    "        dict_folds['test'][subj] = fau_dict[subj]\n",
    "    fold_size = no_train_subjects / no_folds\n",
    "#     fold_size_remainder = no_train_subjects % no_folds\n",
    "    for fold_no in range(no_folds):\n",
    "        fold_subjects = train_subjects[fold_no*fold_size : fold_no*fold_size+fold_size]\n",
    "        dict_folds ['fold_{}'.format(fold_no)]={}\n",
    "        for sub in fold_subjects:\n",
    "            dict_folds ['fold_{}'.format(fold_no)] [sub] = fau_dict [sub]\n",
    "    return dict_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and save images and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function for cropping given an image path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityTransform(inPoints, outPoints) :\n",
    "    s60 = math.sin(60*math.pi/180);\n",
    "    c60 = math.cos(60*math.pi/180);  \n",
    "  \n",
    "    inPts = np.copy(inPoints).tolist();\n",
    "    outPts = np.copy(outPoints).tolist();\n",
    "    \n",
    "    xin = c60*(inPts[0][0] - inPts[1][0]) - s60*(inPts[0][1] - inPts[1][1]) + inPts[1][0];\n",
    "    yin = s60*(inPts[0][0] - inPts[1][0]) + c60*(inPts[0][1] - inPts[1][1]) + inPts[1][1];\n",
    "    \n",
    "    inPts.append([np.int(xin), np.int(yin)]);\n",
    "    \n",
    "    xout = c60*(outPts[0][0] - outPts[1][0]) - s60*(outPts[0][1] - outPts[1][1]) + outPts[1][0];\n",
    "    yout = s60*(outPts[0][0] - outPts[1][0]) + c60*(outPts[0][1] - outPts[1][1]) + outPts[1][1];\n",
    "    \n",
    "    outPts.append([np.int(xout), np.int(yout)]);\n",
    "    \n",
    "    tform = cv2.estimateRigidTransform(np.array([inPts]), np.array([outPts]), False);\n",
    "    \n",
    "    return tform;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new function, doesnt write landmarks every single time\n",
    "def detectAndaligncrop(impath, detector, predictor):\n",
    "    image=cv2.imread(impath)\n",
    "    image_float=np.float32(image)/255.0\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 1)\n",
    "    #initialising images and allPoints arrays\n",
    "    allPoints=[]\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        points=[]\n",
    "        for (x,y) in shape:\n",
    "            points.append((x,y))\n",
    "        allPoints.append(points)\n",
    "    images=[image_float]\n",
    "    #computation\n",
    "    w=112\n",
    "    h=112\n",
    "    eyecornerDst = [ (np.int(0.3 * w ), np.int(h / 3)), (np.int(0.7 * w ), np.int(h / 3)) ];\n",
    "    imagesNorm = [];\n",
    "    pointsNorm = [];\n",
    "    #     print allPoints[0]\n",
    "    # Add boundary points for delaunay triangulation\n",
    "    boundaryPts = np.array([(0,0), (w/2,0), (w-1,0), (w-1,h/2), ( w-1, h-1 ), ( w/2, h-1 ), (0, h-1), (0,h/2) ]);\n",
    "    n = len(allPoints[0]);\n",
    "    numImages = len(images)\n",
    "    for i in xrange(0, numImages):\n",
    "        points1 = allPoints[i];\n",
    "        # Corners of the eye in input image\n",
    "        eyecornerSrc  = [ allPoints[i][36], allPoints[i][45] ] ;\n",
    "        # Compute similarity transform\n",
    "        tform = similarityTransform(eyecornerSrc, eyecornerDst);\n",
    "        # Apply similarity transformation\n",
    "        img = cv2.warpAffine(images[i], tform, (w,h));\n",
    "    #         print(\"debug im type shape max mean min \", img.dtype,img.shape,np.max(img),np.mean(img),np.min(img))\n",
    "    #         plt.imshow(img)\n",
    "        # Apply similarity transform on points\n",
    "        points2 = np.reshape(np.array(points1), (68,1,2));        \n",
    "        points = cv2.transform(points2, tform);\n",
    "        points = np.float32(np.reshape(points, (68, 2)));\n",
    "        pointsNorm.append(points);\n",
    "        imagesNorm.append(img);\n",
    "    #     print (pointsNorm[0])\n",
    "    #     plt.imshow(imagesNorm[0]) \n",
    "    # Output image\n",
    "    output=imagesNorm[0]\n",
    "    rgb_image=cv2.cvtColor(output,cv2.COLOR_BGR2RGB)\n",
    "    return rgb_image, pointsNorm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for getting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting HOG, given an image path or an image, return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in rgb images and returns the required HOG descriptor array. \n",
    "def getHOGFeatures (orientations, pixels_per_cell, cells_per_block, image):\n",
    "    if isinstance(image, basestring):\n",
    "        im = cv2.cvtColor(cv2.imread(image),cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        im = image\n",
    "    gray_im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) \n",
    "    fd, hog_image = hog(gray_im, orientations=orientations, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, visualise=True)\n",
    "#     hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))\n",
    "#     plt.imshow (hog_image_rescaled, cmap = plt.cm.gray)\n",
    "#     print(\"HOG vector dimension: \", fd.shape)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imageTouse=\"files_may20/1.jpeg\"\n",
    "sample_alignedAndCropped,landmarkPoints=detectAndaligncrop(imageTouse, detector, predictor)\n",
    "plt.imshow(sample_alignedAndCropped)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = getHOGFeatures(6, (8,8), (4,4), sample_alignedAndCropped)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "carray_fd = carray(a, rootdir='/home/amogh/m', mode = 'w')\n",
    "carray_fd.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to get features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing functions and function_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAU4_1(image,landmarks):\n",
    "    cropped_im=image[:38]\n",
    "    return cropped_im\n",
    "\n",
    "def FAU1_1(image,landmarks):\n",
    "    cropped_im=image[:38]\n",
    "    return cropped_im\n",
    "\n",
    "def FAU2_1(image,landmarks):\n",
    "    cropped_im=image[:38]\n",
    "    return cropped_im\n",
    "\n",
    "def FAU5_1(image,landmarkPoints): #includes border\n",
    "    rect_top=int(landmarkPoints[17][1])\n",
    "    rect_bottom=int(landmarkPoints[29][1])\n",
    "    rect_left=int(landmarkPoints[3][0])\n",
    "    rect_right=int(landmarkPoints[12][0])\n",
    "    cropped_im=image[rect_top:rect_bottom,rect_left:rect_right]\n",
    "    border_top, border_bottom, border_left, border_right = [0,32-height,0,64-width]\n",
    "    img_with_border = cv2.copyMakeBorder(cropped_im, border_top, border_bottom, border_left, border_right, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "    return img_with_border\n",
    "\n",
    "def FAU12right_1(image,landmarkPoints):\n",
    "    rect_top = int(landmarkPoints[34][1])\n",
    "    rect_bottom = int(landmarkPoints[11][1])\n",
    "    rect_left = int(landmarkPoints[34][0])\n",
    "    rect_right = int(landmarkPoints[11][0])\n",
    "    cropped_im = image[rect_top:rect_bottom,rect_left:rect_right]\n",
    "    border_top, border_bottom, border_left, border_right = [0,32-height,0,32-width]\n",
    "    img_with_border = cv2.copyMakeBorder(cropped_im, border_top, border_bottom, border_left, border_right, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "    return img_with_border\n",
    "\n",
    "def FAU12left_1(image,landmarkPoints):\n",
    "    rect_top = int(landmarkPoints[32][1])\n",
    "    rect_bottom = int(landmarkPoints[5][1])\n",
    "    rect_left = int(landmarkPoints[5][0])\n",
    "    rect_right = int(landmarkPoints[32][0])\n",
    "    cropped_im = image[rect_top:rect_bottom,rect_left:rect_right]\n",
    "    border_top, border_bottom, border_left, border_right = [0,32-height,0,32-width]\n",
    "    img_with_border = cv2.copyMakeBorder(cropped_im, border_top, border_bottom, border_left, border_right, cv2.BORDER_CONSTANT, value=[0,0,0])\n",
    "    return img_with_border\n",
    "\n",
    "function_dict={'FAU1_1':FAU1_1,'FAU2_1':FAU2_1,'FAU4_1':FAU4_1,'FAU5_1':FAU5_1, 'FAU12right_1':FAU12right_1, 'FAU12left_1':FAU12left_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crop and save function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by keeping in mind that these are the parameters that we need to pass: o, ppc cpb, fau_no, thresh, function used for cropping, folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves images and HOG features\n",
    "def cropAndSaveImageHOG (o ,ppc ,cpb ,fau_no , thresh, dict_folds, folder_DISFA_data, cropping_function_name, function_dict, featuresFunction, boolSave=True):\n",
    "    folder_cropped_images = folder_DISFA_data + \"/features/cropped_images/\"\n",
    "    folder_dest = folder_cropped_images +  \"/{}/{}/\".format(thresh,cropping_function_name)\n",
    "    folder_features_dest = folder_DISFA_data + \"/features/hog/{}/{}/\".format(thresh,cropping_function_name)\n",
    "    print(\"images go to: \",folder_dest, \"\\n\", \"features go to:\", folder_features_dest)\n",
    "    # initialize dlib's face detector (HOG-based) and then create\n",
    "    # the facial landmark predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    if not os.path.exists(folder_dest):\n",
    "        os.makedirs(folder_dest)\n",
    "    if not os.path.exists(folder_features_dest):\n",
    "        os.makedirs(folder_features_dest)\n",
    "    for fold in dict_folds.keys():\n",
    "        print (\"inside:\", fold)\n",
    "        for subj in dict_folds[fold]:\n",
    "            for category in dict_folds[fold][subj]:\n",
    "#                 print (\"inside: \",fold,subj,category)\n",
    "                folder_dest_image = folder_dest + \"{}/{}/{}/\".format(fold,subj,category)\n",
    "                folder_dest_feature = folder_features_dest + \"{}/{}/{}/\".format(fold,subj,category)\n",
    "                if not os.path.exists(folder_dest_image):\n",
    "                    os.makedirs(folder_dest_image)\n",
    "                for frame_no, frame in enumerate(dict_folds[fold][subj][category]):\n",
    "                    im_path = folder_DISFA_data + \"Videos_RightCamera/RightVideo{}/{}.jpeg\".format(subj,int(frame))\n",
    "                    im_basename = os.path.basename(im_path)\n",
    "                    im_dest_path = folder_dest_image + im_basename \n",
    "                    features_path = folder_dest_feature + os.path.splitext(im_basename)[0] \n",
    "                    if os.path.exists(im_path):\n",
    "                        try:\n",
    "                            #cropping and aligning images\n",
    "                            im_aligned_cropped,landmarkPoints = detectAndaligncrop(im_path, detector, predictor)\n",
    "                            cropped_rgb_image = function_dict[cropping_function_name] (im_aligned_cropped, landmarkPoints)\n",
    "                            #saving cropped RGB images in BGR(because opencv uses BGR as default)\n",
    "                            cv2.imwrite(im_dest_path, cv2.cvtColor(cropped_rgb_image,cv2.COLOR_RGB2BGR)*255.)\n",
    "                            #getting features\n",
    "                            fd = featuresFunction(o, ppc, cpb, cropped_rgb_image)\n",
    "                            #saving features\n",
    "                            if not (os.path.exists(features_path)):\n",
    "                                os.makedirs(features_path)\n",
    "                            carray_fd = carray(fd, rootdir=features_path, mode = 'w')\n",
    "                            carray_fd.flush()\n",
    "                            if frame_no%100 == 0:\n",
    "                                print(\"frames processed: \", frame_no)\n",
    "                        except KeyboardInterrupt:\n",
    "                            break\n",
    "                        except: \n",
    "                            continue\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "               "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cropAndSaveImageHOG(6,(8,8),(4,4),2, 3, getTrainTestFolds(getDISFAFramesDictionary(folder_DISFA_FAU_summary,2,3),5,2), folder_DISFA_data,'FAU2_1',function_dict,getHOGFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalSaveImagesFeatures(o ,ppc ,cpb ,fau_no , thresh, cropping_function_name, no_folds=5, no_test_subjects=2, function_dict=function_dict, featuresFunction=getHOGFeatures, folder_DISFA_FAU_summary=folder_DISFA_FAU_summary, folder_DISFA_data=folder_DISFA_data, boolEqualise=True):\n",
    "    frames_dict = getDISFAFramesDictionary(folder_DISFA_FAU_summary,fau_no,thresh)\n",
    "    frames_dict = equaliseDictionary(frames_dict)\n",
    "    dict_folds = getTrainTestFolds(frames_dict,no_folds,no_test_subjects)\n",
    "    cropAndSaveImageHOG(o ,ppc ,cpb ,fau_no ,thresh , dict_folds, folder_DISFA_data,cropping_function_name,function_dict,getHOGFeatures)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "finalSaveImagesFeatures (6 ,(8,8) ,(4,4) ,2 , 3, 'FAU2_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "    - just calculate the features and save them in appropriate folder; save colored image only so that you can use deep learning\n",
    "    - for training; load the features and make X, Y. Then train for different folds, report accuracy for each test fold and show the average in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDISFA (fau_no, train_no, fau_thresh, test_subjects_no, boolGetLists=False, boolCalcFeatures=False, boolCrossValidation=True, ):\n",
    "    if boolGetLists:\n",
    "        getDISFALists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainSVMGridSearchModel helper function\n",
    "Using GridSearchCV model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to use custom cross validation generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to train once custom iterable, train and the test function have been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVMGridSearchModel(X_train, Y_train , no_jobs=1, kernel_list=['rbf','linear'], custom_fold_iterable):\n",
    "    #setup parameter search space\n",
    "    gamma_range = np.outer(np.logspace(-3,0,4),np.array([1,5]))\n",
    "    gamma_range = gamma_range.flatten()\n",
    "    C_range = np.outer(np.logspace(-1,1,3),np.array([1,5]))\n",
    "    C_range = C_range.flatten()\n",
    "    parameters = {'kernel': kernel_list,'C':C_range,'gamma':gamma_range}\n",
    "    svm_clsf = svm.SVC()\n",
    "    grid_clsf = sklearn.model_selection.GridSearchCV(estimator=svm_clsf,param_grid=parameters,n_jobs=no_jobs,verbose=2,cv=custom_fold_iterable)\n",
    "    #train\n",
    "    start_time=dt.datetime.now()\n",
    "    print('Start param searching at {}'.format(str(start_time)))\n",
    "    grid_clsf.fit(X_train,Y_train)\n",
    "    elapsed_time=dt.datetime.now()-start_time\n",
    "    print('Elapsed time, param searching {}'.format(str(elapsed_time)))\n",
    "    sorted(grid_clsf.cv_results_.keys())\n",
    "    return grid_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCustomGridSearch(fau_no, thresh, cropping_function_name,trainFunction, folder_data=folder_DISFA_data):\n",
    "    \n",
    "    fold_folder_list = glob.glob(folder_data + \"features/hog/{}/{}/*\".format(thresh,cropping_function_name))\n",
    "    \n",
    "    # defining global holders and variables\n",
    "    no_folds = len(fold_folder_list)\n",
    "    features = []\n",
    "    targets = []\n",
    "    fold_label_list = []\n",
    "\n",
    "    #processing for each fold:\n",
    "    for fold_no, fol in enumerate(fold_folder_list):\n",
    "        \n",
    "        #lists specific to fold\n",
    "        list_positive_feature_folders = []\n",
    "        list_negative_feature_folders = []\n",
    "        positive_features = []\n",
    "        negative_features = []\n",
    "        fold_targets = []\n",
    "        fold_train_features = []\n",
    "        \n",
    "        #loading features in lists\n",
    "        list_positive_feature_folders.extend(glob.glob(fol + \"/*/positives/*/\"))\n",
    "        list_negative_feature_folders.extend(glob.glob(fol + \"/*/negatives/*/\"))\n",
    "        print(\"loading positive features for fold: \", fold_no)\n",
    "        for pos_feat_folder in list_positive_feature_folders:\n",
    "            pos_feat = carray(rootdir = pos_feat_folder, mode = 'r')\n",
    "            positive_features.append(pos_feat)\n",
    "        print(\"loading negative features for fold: \", fold_no)\n",
    "        for neg_feat_folder in list_negative_feature_folders:\n",
    "            neg_feat = carray(rootdir = neg_feat_folder, mode = 'r')\n",
    "            negative_features.append(neg_feat)\n",
    "\n",
    "        fold_train_features.extend(positive_features)\n",
    "        fold_train_features.extend(negative_features)\n",
    "        fold_targets.extend([1] * len(positive_features))\n",
    "        fold_targets.extend([0] * len(negative_features))\n",
    "        no_fold_features = len(positive_features) + len(negative_features)\n",
    "        print(\"this fold has these many features: \",no_fold_features)\n",
    "        \n",
    "        #updating global features and targets\n",
    "        features.extend(fold_train_features)\n",
    "        targets.extend(fold_targets)\n",
    "        #updating fold_label_list\n",
    "        fold_label_list.extend([fold_no]*no_fold_features)\n",
    "\n",
    "    #defining the custom cross validation generator over training data\n",
    "    cvIterable= []\n",
    "    for fold_no in range(no_folds):\n",
    "        fold_label_list = np.array(fold_label_list)\n",
    "        train_indices = np.argwhere(fold_label_list != fold_no).flatten()\n",
    "        test_indices = np.argwhere(fold_label_list == fold_no).flatten()\n",
    "        cvIterable.append((train_indices,test_indices))\n",
    "    \n",
    "    classifier_results = trainSVMGridSearchModel(features ,targets ,no_jobs=8 , kernel_list=['linear'], cvIterable)\n",
    "    \n",
    "    return classifier_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Rough functions and ideas(not useful now)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "gamma_range = np.outer(np.logspace(-3,0,4),np.array([1,5]))\n",
    "gamma_range = gamma_range.flatten()\n",
    "C_range = np.outer(np.logspace(-1,1,3),np.array([1,5]))\n",
    "C_range = C_range.flatten()\n",
    "parameters = {'kernel': ['linear'],'C':C_range,'gamma':gamma_range}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "for g in sklearn.model_selection.ParameterGrid(parameters):\n",
    "    print (g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def trainSVMGridSearchModel(X_train, Y_train, X_test, Y_test, no_jobs=4, kernel_list=['linear']):\n",
    "    #setup parameter search space\n",
    "    gamma_range = np.outer(np.logspace(-3,0,4),np.array([1,5]))\n",
    "    gamma_range = gamma_range.flatten()\n",
    "    C_range = np.outer(np.logspace(-1,1,3),np.array([1,5]))\n",
    "    C_range = C_range.flatten()\n",
    "    parameters = {'kernel': kernel_list,'C':C_range,'gamma':gamma_range}\n",
    "    for g in sklearn.model_selection.ParameterGrid(parameters):\n",
    "            svm_clsf = svm.SVC(        )\n",
    "    svm_clsf = svm.SVC()\n",
    "    grid_clsf = sklearn.model_selection.GridSearchCV(estimator=svm_clsf,param_grid=parameters,n_jobs=no_jobs,verbose=2, cv=[(slice(None), slice(None))])\n",
    "    #train\n",
    "    start_time=dt.datetime.now()\n",
    "    print('Start param searching at {}'.format(str(start_time)))\n",
    "    grid_clsf.fit(X_train,Y_train)\n",
    "    elapsed_time=dt.datetime.now()-start_time\n",
    "    print('Elapsed time, param searching {}'.format(str(elapsed_time)))\n",
    "    sorted(grid_clsf.cv_results_.keys())\n",
    "    return grid_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Main Train Function for manual fold approach(not useful now)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# TO BE ABLE TO TAKE IN OTHER DATASETS ALSO, SET THE FOLDER DATA ACCORDINGLY IN THE CORRESPONDING FUNCTION TO THE DATASET\n",
    "#ideally loading folds also should've been a function.\n",
    "#trainFunction as an argument allows changing choice of function for choosing the model\n",
    "def train(fau_no, thresh, cropping_function_name,trainFunction, folder_data):\n",
    "    fold_folder_list = glob.glob(folder_data + \"features/hog/{}/{}/*\".format(thresh, cropping_function_name))\n",
    "    no_folds = len(fold_folder_list)\n",
    "    for fold_no in range(no_folds):\n",
    "        print(\"In fold number\", fold_no)\n",
    "        #do fold_no times training by testing on the folder corresponding to fold_no\n",
    "        train_folds_folder_list = fold_folder_list[:fold_no] + fold_folder_list[fold_no+1:]\n",
    "        list_positive_feature_folders = []\n",
    "        list_negative_feature_folders = []\n",
    "        positive_features = []\n",
    "        negative_features = []\n",
    "        # populate the list_positive_feature_folders and list_negative_feature_folders\n",
    "        for fol in train_folds_folder_list:\n",
    "            list_positive_feature_folders.extend(glob.glob(fol + \"/*/positives/*/\"))\n",
    "            list_negative_feature_folders.extend(glob.glob(fol + \"/*/negatives/*/\"))\n",
    "        # populate the positive_features and negative_features array\n",
    "        print(\"loading positive features\")\n",
    "        for pos_feat_folder in list_positive_feature_folders:\n",
    "            pos_feat = carray(rootdir = pos_feat_folder, mode = 'r')\n",
    "            positive_features.append(pos_feat)\n",
    "        print(\"loading negative features\")\n",
    "        for neg_feat_folder in list_negative_feature_folders:\n",
    "            neg_feat = carray(rootdir = neg_feat_folder, mode = 'r')\n",
    "            negative_features.append(neg_feat)\n",
    "        positive_features = np.array(positive_features)\n",
    "        negative_features = np.array(negative_features)\n",
    "        print(\"shape of positive features array is: \", (positive_features).shape)\n",
    "        print(\"shape of negative features array is: \", (negative_features).shape)\n",
    "        train_array_X = np.concatenate((positive_features,negative_features))\n",
    "        target_positives = np.ones(positive_features.shape[0])\n",
    "        target_negatives = np.zeros(negative_features.shape[0])\n",
    "        targets_Y = np.append(target_positives, target_negatives)\n",
    "        # training data and labels loaded.\n",
    "\n",
    "        #loading test data\n",
    "        test_folder = fold_folder_list[fold_no]\n",
    "        list_positive_test_folder = glob.glob(test_folder + \"/*/positives/*/\")\n",
    "        list_negative_test_folder = glob.glob(test_folder + \"/*/negatives/*/\")\n",
    "        positive_test_features = []\n",
    "        negative_test_features = []\n",
    "        for pos_feat_folder in list_positive_test_folder:\n",
    "        pos_feat = carray(rootdir = pos_feat_folder, mode = 'r')\n",
    "        positive_test_features.append(pos_feat)\n",
    "        for neg_feat_folder in list_negative_test_folder:\n",
    "            neg_feat = carray(rootdir = neg_feat_folder, mode = 'r')\n",
    "            negative_test_features.append(neg_feat)\n",
    "        positive_test_features = np.array(positive_test_features)\n",
    "        negative_test_features = np.array(negative_test_features)\n",
    "        test_array_X = np.concatenate((positive_test_features,negative_test_features))\n",
    "        test_array_Y = np.append(np.ones(positive_test_features.shape[0]), np.zeros(negative_test_features.shape[0]))\n",
    "        print(\"shape of test array X: \", np.array(test_array_X).shape)\n",
    "        print(\"shape of test array Y: \", np.array(test_array_Y).shape)\n",
    "    \n",
    "        # training for this fold\n",
    "        results = {}\n",
    "        trainFunction(train_array_X, train_targets_Y, test_array_X, test_array_Y)\n",
    "        classificationResult = trainModel(train_array_X, targets_Y, no_jobs, kernel_list = ['linear'])\n",
    "        best_classifier = grid_clsf.best_estimator_\n",
    "        best_params=grid_clsf.best_params_\n",
    "        scores=grid_clsf.cv_results_['mean_test_score'].reshape(2,len(C_range),len(gamma_range))\n",
    "        print(\"scores are: \", scores)\n",
    "        print(\"best classifier is:\", \"\\n\", classifier)\n",
    "        print(\"best parameters are: \", \"\\n\", best_params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "fold_folder_list = glob.glob(folder_DISFA_data + \"features/hog/{}/{}/*\".format(3,'FAU2_1'))\n",
    "no_folds = len(fold_folder_list)\n",
    "for fold_no in range(no_folds):\n",
    "    print(\"In fold number\", fold_no)\n",
    "    #do fold_no times training by testing on the folder corresponding to fold_no\n",
    "    train_folds_folder_list = fold_folder_list[:fold_no] + fold_folder_list[fold_no+1:]\n",
    "    list_positive_feature_folders = []\n",
    "    list_negative_feature_folders = []\n",
    "    positive_features = []\n",
    "    negative_features = []\n",
    "    # populate the list_positive_feature_folders and list_negative_feature_folders\n",
    "    for fol in train_folds_folder_list:\n",
    "        list_positive_feature_folders.extend(glob.glob(fol + \"/*/positives/*/\"))\n",
    "        list_negative_feature_folders.extend(glob.glob(fol + \"/*/negatives/*/\"))\n",
    "    # populate the positive_features and negative_features array\n",
    "    print(\"loading positive features\")\n",
    "    for pos_feat_folder in list_positive_feature_folders:\n",
    "        pos_feat = carray(rootdir = pos_feat_folder, mode = 'r')\n",
    "        positive_features.append(pos_feat)\n",
    "    print(\"loading negative features\")\n",
    "    for neg_feat_folder in list_negative_feature_folders:\n",
    "        neg_feat = carray(rootdir = neg_feat_folder, mode = 'r')\n",
    "        negative_features.append(neg_feat)\n",
    "    positive_features = np.array(positive_features)\n",
    "    negative_features = np.array(negative_features)\n",
    "    print(\"shape of np positive arrays is: \", (positive_features).shape)\n",
    "    print(\"shape of np negative arrays is: \", (negative_features).shape)\n",
    "    train_array_X = np.concatenate((positive_features,negative_features))\n",
    "    train_targets_Y = np.append(np.ones(positive_features.shape[0]), np.zeros(negative_features.shape[0]))\n",
    "    #loading test data\n",
    "    test_folder = fold_folder_list[fold_no]\n",
    "    list_positive_test_folder = glob.glob(test_folder + \"/*/positives/*/\")\n",
    "    list_negative_test_folder = glob.glob(test_folder + \"/*/negatives/*/\")\n",
    "    positive_test_features = []\n",
    "    negative_test_features = []\n",
    "    for pos_feat_folder in list_positive_test_folder:\n",
    "        pos_feat = carray(rootdir = pos_feat_folder, mode = 'r')\n",
    "        positive_test_features.append(pos_feat)\n",
    "    for neg_feat_folder in list_negative_test_folder:\n",
    "        neg_feat = carray(rootdir = neg_feat_folder, mode = 'r')\n",
    "        negative_test_features.append(neg_feat)\n",
    "    positive_test_features = np.array(positive_test_features)\n",
    "    negative_test_features = np.array(negative_test_features)\n",
    "    test_array_X = np.concatenate((positive_test_features,negative_test_features))\n",
    "    test_array_Y = np.append(np.ones(positive_test_features.shape[0]), np.zeros(negative_test_features.shape[0]))\n",
    "    print(\"shape of test array X: \", np.array(test_array_X).shape)\n",
    "    print(\"shape of test array Y: \", np.array(test_array_Y).shape)\n",
    "#     positives = []\n",
    "#     for fold_folder in train_folder_list:\n",
    "#         array_posi=carray(rootdir=dir_features_hog_1_fau4_1+'positives/',mode='r')\n",
    "#         print(fold_foder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
